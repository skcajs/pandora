{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71aec365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 08:41:05.547658: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746600065.597458    5747 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746600065.612113    5747 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746600065.721615    5747 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746600065.721633    5747 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746600065.721640    5747 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746600065.721641    5747 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-07 08:41:05.734495: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import pprint\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from sbi import utils as sbi_utils\n",
    "from sbi.inference import NPE, simulate_for_sbi\n",
    "from sbi.analysis import pairplot\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")\n",
    "\n",
    "from sbi.neural_nets import posterior_nn\n",
    "from sbi.inference import NPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiphase_tank(theta):\n",
    "    \"\"\"\n",
    "    Simplified tank model for multiphase flow.\n",
    "\n",
    "    theta: [k_P, k_alpha, Q_g, Q_l]\n",
    "    \"\"\"\n",
    "    k_P, k_alpha, Q_g, Q_l = theta\n",
    "    \n",
    "    Q_out = 0.05  # fixed known outflow\n",
    "    P0 = 5e6      # initial pressure (Pa)\n",
    "    alpha0 = 0.2  # initial gas fraction\n",
    "\n",
    "    def rhs(t, y):\n",
    "        P, alpha = y\n",
    "        \n",
    "        dP_dt = k_P * (Q_g - alpha * Q_out)\n",
    "        d_alpha_dt = k_alpha * ((Q_g / (Q_g + Q_l)) - alpha)\n",
    "        \n",
    "        return [dP_dt, d_alpha_dt]\n",
    "\n",
    "    t_span = (0, 200)\n",
    "    t_eval = np.linspace(*t_span, 200)\n",
    "    sol = solve_ivp(rhs, t_span, [P0, alpha0], t_eval=t_eval)\n",
    "\n",
    "    if not sol.success:\n",
    "        raise RuntimeError(\"Simulation failed\")\n",
    "\n",
    "    return sol.y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85942283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiphase_tank_summary(theta):\n",
    "    output = multiphase_tank(theta).flatten()\n",
    "    P, alpha = output[:200], output[200:]\n",
    "\n",
    "    def slope(y):\n",
    "        return (y[-1] - y[0]) / 200\n",
    "    \n",
    "    def time_to_equilibrium(y, tol=1e-2):\n",
    "        for i in range(1, len(y)):\n",
    "            if np.abs(y[i] - y[-1]) < tol:\n",
    "                return i\n",
    "        return len(y)\n",
    "\n",
    "    stats = np.array([\n",
    "        np.mean(P), np.std(P), P[-1], slope(P), time_to_equilibrium(P),\n",
    "        np.mean(alpha), np.std(alpha), alpha[-1], slope(alpha), time_to_equilibrium(alpha),\n",
    "    ])\n",
    "\n",
    "    return stats\n",
    "\n",
    "def multiphase_tank_summary_normalized(theta):\n",
    "    raw_stats = multiphase_tank_summary(theta)\n",
    "    normalized_stats = (raw_stats - stats_mean) / stats_std\n",
    "    return normalized_stats\n",
    "\n",
    "\n",
    "def multiphase_tank_summary_snapshots(theta):\n",
    "    output = multiphase_tank(theta).flatten()\n",
    "    P, alpha = output[:200], output[200:]\n",
    "\n",
    "    # Choose snapshot time points\n",
    "    t0, t1, t2, t3, t4, t5 = 1, 20, 50, 100, 150, 199  # early, mid, late\n",
    "\n",
    "    stats = np.array([\n",
    "        P[t0], P[t1], P[t2], P[t3], P[t4], P[t5],\n",
    "        alpha[t0], alpha[t1], alpha[t2],  alpha[t3], alpha[t4], alpha[t5]\n",
    "    ])\n",
    "\n",
    "    return stats\n",
    "\n",
    "def multiphase_tank_for_cnn(theta):\n",
    "    output = multiphase_tank(theta)  # shape: (400,)\n",
    "    output = output.reshape(2, 200)  # [channels, time]\n",
    "    return output.astype(np.float32)\n",
    "\n",
    "def multiphase_tank_normalized(theta):\n",
    "    output = multiphase_tank(theta).reshape(2, 200)\n",
    "    output[0] = (output[0] - output[0].mean()) / (output[0].std() + 1e-8)  # Normalize P\n",
    "    output[1] = (output[1] - output[1].mean()) / (output[1].std() + 1e-8)  # Normalize alpha\n",
    "    return output.astype(np.float32)\n",
    "\n",
    "# Alpha only simulator\n",
    "def multiphase_tank_alpha_only(theta):\n",
    "    output = multiphase_tank(theta).reshape(2, 200)\n",
    "    alpha = output[1]\n",
    "    alpha = (alpha - alpha.mean()) / (alpha.std() + 1e-8)\n",
    "    return alpha[np.newaxis, :].astype(np.float32)  # shape (1, 200)\n",
    "\n",
    "def multiphase_tank_pressure_only(theta):\n",
    "    output = multiphase_tank(theta).reshape(2, 200)\n",
    "    pressure = output[0]\n",
    "    pressure = (pressure - pressure.mean()) / (pressure.std() + 1e-8)\n",
    "    return pressure[np.newaxis, :].astype(np.float32)  # shape (1, 200)\n",
    "\n",
    "def multiphase_tank_hybrid(theta):\n",
    "    seq = multiphase_tank_normalized(theta).reshape(2, 200)\n",
    "    stats = multiphase_tank_summary(theta)\n",
    "\n",
    "    # Flatten both into one long vector\n",
    "    hybrid_vector = np.concatenate([seq.flatten(), stats], axis=0)\n",
    "    return hybrid_vector.astype(np.float32)\n",
    "\n",
    "\n",
    "def multiphase_tank_hybrid_normalized(theta):\n",
    "    seq = multiphase_tank_normalized(theta).reshape(2, 200)\n",
    "    stats = multiphase_tank_summary_normalized(theta)\n",
    "\n",
    "    # Flatten both into one long vector\n",
    "    hybrid_vector = np.concatenate([seq.flatten(), stats], axis=0)\n",
    "    return hybrid_vector.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_net = nn.Sequential(\n",
    "    nn.Linear(400, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 10)  # Final embedded representation\n",
    ")\n",
    "\n",
    "embedding_net_single = nn.Sequential(\n",
    "    nn.Conv1d(1, 32, kernel_size=7, padding=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv1d(32, 64, kernel_size=7, padding=3),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool1d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 10)\n",
    ")\n",
    "\n",
    "embedding_net_large = nn.Sequential(\n",
    "    nn.Conv1d(2, 32, kernel_size=7, padding=3),\n",
    "    nn.ReLU(),\n",
    "    nn.LayerNorm([32, 200]),\n",
    "    nn.Conv1d(32, 64, kernel_size=7, padding=3),\n",
    "    nn.ReLU(),\n",
    "    nn.LayerNorm([64, 200]),\n",
    "    nn.Conv1d(64, 128, kernel_size=7, padding=3),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool1d(1),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(32, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f60cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEmbedding(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=64, output_size=10, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,  # 2 = pressure + alpha\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, channels, time]\n",
    "        x = x.permute(0, 2, 1)  # -> [batch_size, time, channels]\n",
    "        _, (hn, _) = self.lstm(x)  # hn shape: [num_layers, batch, hidden]\n",
    "        embedding = hn[-1]  # take last layer's hidden state\n",
    "        return self.output_net(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55325abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEmbedding(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=64, output_size=10, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, hn = self.gru(x)\n",
    "        embedding = hn[-1]\n",
    "        return self.output_net(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb081070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, input_channels=2, seq_len=200, d_model=64, nhead=4, num_layers=2, output_size=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Conv1d(input_channels, d_model, kernel_size=1)  # [B, C=2, T] -> [B, d_model, T]\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, d_model))      # Learnable positional encoding\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=128, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C=2, T=200] => [B, d_model, T]\n",
    "        x = self.input_proj(x)                   # [B, d_model, T]\n",
    "        x = x.permute(0, 2, 1)                   # [B, T, d_model]\n",
    "        x = x + self.pos_encoding[:x.size(1)]    # Add positional encoding\n",
    "\n",
    "        x = self.transformer(x)                  # [B, T, d_model]\n",
    "        x = x.mean(dim=1)                        # Global average pooling over time\n",
    "\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8084d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridEmbedding(nn.Module):\n",
    "    def __init__(self, seq_embed_net, stats_dim=10, seq_channels=2, seq_len=200, fused_dim=10):\n",
    "        super().__init__()\n",
    "        self.seq_embed_net = seq_embed_net\n",
    "        self.stats_dim = stats_dim\n",
    "        self.seq_shape = (seq_channels, seq_len)\n",
    "\n",
    "        self.stats_fc = nn.Sequential(\n",
    "            nn.Linear(stats_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(16 + fused_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x is a [batch, 2*200 + 10] flat tensor\n",
    "        # Split into sequence and stats\n",
    "        seq_flat = x[:, :-self.stats_dim]\n",
    "        stats = x[:, -self.stats_dim:]\n",
    "\n",
    "        seq = seq_flat.view(x.shape[0], *self.seq_shape)  # Reshape to [B, 2, 200]\n",
    "\n",
    "        seq_embed = self.seq_embed_net(seq)               # e.g., LSTM embedding\n",
    "        stats_embed = self.stats_fc(stats)\n",
    "\n",
    "        combined = torch.cat([seq_embed, stats_embed], dim=1)\n",
    "        return self.fusion(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior2 = sbi_utils.BoxUniform(\n",
    "    low=torch.tensor([2e4, 0.05, 0.003, 0.025]),\n",
    "    high=torch.tensor([8e4, 0.2, 0.007, 0.035])\n",
    ")\n",
    "prior1 = sbi_utils.BoxUniform(\n",
    "    low=torch.tensor([2e4, 0.01, 0.001, 0.01]),\n",
    "    high=torch.tensor([8e4, 1.0, 0.01, 0.05])\n",
    ")\n",
    "prior0 = sbi_utils.BoxUniform(\n",
    "    low=torch.tensor([1e3, 0.01, 0.001, 0.01]),\n",
    "    high=torch.tensor([1e5, 1.0, 0.01, 0.05])\n",
    ")\n",
    "\n",
    "prior = prior1\n",
    "\n",
    "num_samples = 1000000\n",
    "num_rounds = 2\n",
    "num_simulations = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddbb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_example = prior.sample()\n",
    "output = multiphase_tank(theta_example).flatten()\n",
    "P, alpha = output[:200], output[200:]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "t = np.linspace(0, 100, 200)\n",
    "\n",
    "plt.plot(t, P, label=\"Pressure (Pa)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(t, alpha, label=\"Gas Volume Fraction\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f03fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior, num_parameters, prior_returns_numpy = process_prior(prior)\n",
    "\n",
    "simulator_wrapper = process_simulator(\n",
    "    multiphase_tank_summary,\n",
    "    prior,\n",
    "    prior_returns_numpy\n",
    ")\n",
    "\n",
    "check_sbi_inputs(simulator_wrapper, prior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623dd1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using simulator_wrapper and prior defined above\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=num_simulations)\n",
    "inference = NPE(prior=prior)\n",
    "density_estimator = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior(density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic observation\n",
    "# true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define density estimator using the embedding net\n",
    "density_estimator = posterior_nn(model='maf', embedding_net=embedding_net)\n",
    "inference = NPE(prior=prior, density_estimator=density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcfdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_wrapper = process_simulator(\n",
    "    multiphase_tank,\n",
    "    prior,\n",
    "    prior_returns_numpy\n",
    ")\n",
    "\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=1000)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966fe05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic observation\n",
    "# true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffbb123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define density estimator using the embedding net\n",
    "density_estimator = posterior_nn(model='maf', embedding_net=embedding_net_large)\n",
    "inference = NPE(prior=prior, density_estimator=density_estimator)\n",
    "\n",
    "simulator_wrapper = process_simulator(\n",
    "    multiphase_tank_for_cnn,\n",
    "    prior,\n",
    "    prior_returns_numpy\n",
    ")\n",
    "\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=1000)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic observation\n",
    "# true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define density estimator using the embedding net\n",
    "density_estimator = posterior_nn(model='maf', embedding_net=embedding_net_large)\n",
    "inference = NPE(prior=prior, density_estimator=density_estimator)\n",
    "\n",
    "simulator_wrapper = process_simulator(\n",
    "    multiphase_tank_normalized,\n",
    "    prior,\n",
    "    prior_returns_numpy\n",
    ")\n",
    "\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=num_simulations)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed79c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic observation\n",
    "# true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "posterior_samples = posterior.sample((1,), x=x_o)\n",
    "theta_example = posterior_samples\n",
    "\n",
    "output = simulator_wrapper(theta_example).flatten()\n",
    "\n",
    "\n",
    "P, alpha = output[:200], output[200:]\n",
    "P_t, alpha_t = x_o.flatten()[:200],  x_o.flatten()[200:]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "t = np.linspace(0, 100, 200)\n",
    "\n",
    "plt.plot(t, P, label=\"Pressure (Pa) - Simulated\")\n",
    "plt.plot(t, P_t, label=\"Pressure (Pa) - Observed\", linestyle=\"--\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(t, alpha, label=\"Gas Volume Fraction - Simulated\")\n",
    "plt.plot(t, alpha_t, label=\"Gas Volume Fraction - Observed\", linestyle=\"--\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc557a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of posterior_samples along axis 0\n",
    "posterior_std = posterior_samples.std(dim=0)\n",
    "\n",
    "result = {label: f\"{value:.4f} ± {std:.4f}\" for label, value, std in zip(param_labels, posterior_samples[0].tolist(), posterior_std.tolist())}\n",
    "pprint.pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f543e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define density estimator using the embedding net for single channel alpha only\n",
    "density_estimator = posterior_nn(model='maf', embedding_net=embedding_net_single)\n",
    "inference = NPE(prior=prior, density_estimator=density_estimator)\n",
    "\n",
    "simulator_wrapper = process_simulator(\n",
    "    multiphase_tank_alpha_only,\n",
    "    prior,\n",
    "    prior_returns_numpy\n",
    ")\n",
    "\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=10000)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cae876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic observation for known theta\n",
    "true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "# true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95091bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_example = posterior.sample((1,), x=x_o)\n",
    "\n",
    "output = simulator_wrapper(theta_example).flatten()\n",
    "\n",
    "\n",
    "P, alpha = output[:200], output[200:]\n",
    "P_t, alpha_t = x_o.flatten()[:200],  x_o.flatten()[200:]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "t = np.linspace(0, 100, 200)\n",
    "\n",
    "plt.plot(t, P, label=\"Gas Volume Fraction - Simulated\")\n",
    "plt.plot(t, P_t, label=\"Gas Volume Fraction - Observed\", linestyle=\"--\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of posterior_samples along axis 0\n",
    "posterior_std = posterior_samples.std(dim=0)\n",
    "\n",
    "result = {label: f\"{value:.4f} ± {std:.4f}\" for label, value, std in zip(param_labels, posterior_samples[0].tolist(), posterior_std.tolist())}\n",
    "pprint.pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26495071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define density estimator using the embedding net for single channel alpha only\n",
    "density_estimator = posterior_nn(model='maf', embedding_net=embedding_net_single)\n",
    "inference = NPE(prior=prior, density_estimator=density_estimator)\n",
    "\n",
    "simulator_wrapper = process_simulator(\n",
    "    multiphase_tank_pressure_only,\n",
    "    prior,\n",
    "    prior_returns_numpy\n",
    ")\n",
    "\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=10000)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c939ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic observation for known theta\n",
    "# true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140fc5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theta_example = posterior.sample((1,), x=x_o)\n",
    "\n",
    "output = simulator_wrapper(theta_example).flatten()\n",
    "\n",
    "\n",
    "P, alpha = output[:200], output[200:]\n",
    "P_t, alpha_t = x_o.flatten()[:200],  x_o.flatten()[200:]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "t = np.linspace(0, 100, 200)\n",
    "\n",
    "plt.plot(t, P, label=\"Pressure (Pa) - Simulated\")\n",
    "plt.plot(t, P_t, label=\"Pressure (Pa) - Observed\", linestyle=\"--\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44864cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of posterior_samples along axis 0\n",
    "posterior_std = posterior_samples.std(dim=0)\n",
    "\n",
    "result = {label: f\"{value:.4f} ± {std:.4f}\" for label, value, std in zip(param_labels, posterior_samples[0].tolist(), posterior_std.tolist())}\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEmbedding(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=64, output_size=10, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,  # 2 = pressure + alpha\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, channels, time]\n",
    "        x = x.permute(0, 2, 1)  # -> [batch_size, time, channels]\n",
    "        _, (hn, _) = self.lstm(x)  # hn shape: [num_layers, batch, hidden]\n",
    "        embedding = hn[-1]  # take last layer's hidden state\n",
    "        return self.output_net(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea39976",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_net = LSTMEmbedding()\n",
    "\n",
    "# Define density estimator using the embedding net for single channel alpha only\n",
    "density_estimator = posterior_nn(model='maf', embedding_net=embedding_net)\n",
    "inference = NPE(prior=prior, density_estimator=density_estimator)\n",
    "\n",
    "simulator_wrapper = process_simulator(\n",
    "    multiphase_tank_normalized,\n",
    "    prior,\n",
    "    prior_returns_numpy\n",
    ")\n",
    "\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=num_simulations)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ec2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic observation for known theta\n",
    "true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "# true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6163032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of posterior_samples along axis 0\n",
    "posterior_std = posterior_samples.std(dim=0)\n",
    "\n",
    "result = {label: f\"{value:.4f} ± {std:.4f}\" for label, value, std in zip(param_labels, posterior_samples[0].tolist(), posterior_std.tolist())}\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_net = GRUEmbedding()\n",
    "\n",
    "# Define density estimator using the embedding net for single channel alpha only\n",
    "density_estimator = posterior_nn(model='maf', embedding_net=embedding_net)\n",
    "inference = NPE(prior=prior, density_estimator=density_estimator)\n",
    "\n",
    "simulator_wrapper = process_simulator(\n",
    "    multiphase_tank_normalized,\n",
    "    prior,\n",
    "    prior_returns_numpy\n",
    ")\n",
    "\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=num_simulations)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b5937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic observation for known theta\n",
    "true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "# true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3bf269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_net = TransformerEmbedding()\n",
    "\n",
    "# # Define density estimator using the embedding net for single channel alpha only\n",
    "# density_estimator = posterior_nn(model='maf', embedding_net=embedding_net)\n",
    "# inference = NPE(prior=prior, density_estimator=density_estimator)\n",
    "\n",
    "# simulator_wrapper = process_simulator(\n",
    "#     multiphase_tank_normalized,\n",
    "#     prior,\n",
    "#     prior_returns_numpy\n",
    "# )\n",
    "\n",
    "# theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=20000)\n",
    "# _ = inference.append_simulations(theta, x).train()\n",
    "# posterior = inference.build_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ce0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic observation for known theta\n",
    "true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "# true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_net = LSTMEmbedding()\n",
    "embedding_net = HybridEmbedding(seq_embed_net=sequence_net)\n",
    "\n",
    "# Define density estimator using the embedding net for single channel alpha only\n",
    "density_estimator = posterior_nn(model='maf', embedding_net=embedding_net)\n",
    "inference = NPE(prior=prior, density_estimator=density_estimator)\n",
    "\n",
    "simulator_wrapper = process_simulator(\n",
    "    multiphase_tank_hybrid,\n",
    "    prior,\n",
    "    prior_returns_numpy\n",
    ")\n",
    "\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=num_simulations)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic observation for known theta\n",
    "# true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of posterior_samples along axis 0\n",
    "posterior_std = posterior_samples.std(dim=0)\n",
    "\n",
    "result = {label: f\"{value:.4f} ± {std:.4f}\" for label, value, std in zip(param_labels, posterior_samples[0].tolist(), posterior_std.tolist())}\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_net = LSTMEmbedding()\n",
    "embedding_net = HybridEmbedding(seq_embed_net=sequence_net)\n",
    "\n",
    "# Define density estimator using the embedding net for single channel alpha only\n",
    "density_estimator = posterior_nn(model='maf', embedding_net=embedding_net)\n",
    "inference = NPE(prior=prior, density_estimator=density_estimator)\n",
    "\n",
    "# Before training\n",
    "stats_list = []\n",
    "for _ in range(1000):  # Or match num_simulations\n",
    "    theta_i = prior.sample()\n",
    "    stats_i = multiphase_tank_summary(theta_i.numpy())\n",
    "    stats_list.append(stats_i)\n",
    "\n",
    "stats_array = np.stack(stats_list)\n",
    "stats_mean = stats_array.mean(axis=0)\n",
    "stats_std = stats_array.std(axis=0) + 1e-8  # avoid div by 0\n",
    "\n",
    "simulator_wrapper = process_simulator(\n",
    "    multiphase_tank_hybrid_normalized,\n",
    "    prior,\n",
    "    prior_returns_numpy\n",
    ")\n",
    "\n",
    "\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=20000)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic observation for known theta\n",
    "true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "# true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20cc49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_net = LSTMEmbedding()\n",
    "embedding_net = HybridEmbedding(seq_embed_net=sequence_net)\n",
    "\n",
    "# Define density estimator using the embedding net for single channel alpha only\n",
    "density_estimator = posterior_nn(model='maf', embedding_net=embedding_net)\n",
    "inference = NPE(prior=prior, density_estimator=density_estimator)\n",
    "\n",
    "simulator_wrapper = process_simulator(\n",
    "    multiphase_tank_hybrid,\n",
    "    prior,\n",
    "    prior_returns_numpy\n",
    ")\n",
    "\n",
    "\n",
    "theta, x = simulate_for_sbi(simulator_wrapper, prior, num_simulations=1000)\n",
    "_ = inference.append_simulations(theta, x).train()\n",
    "posterior = inference.build_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Synthetic observation for known theta\n",
    "true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "# true_theta = np.array([prior.sample()])\n",
    "x_o = simulator_wrapper(true_theta)\n",
    "\n",
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((num_samples,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f36249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the standard deviation of posterior_samples along axis 0\n",
    "posterior_std = posterior_samples.std(dim=0)\n",
    "\n",
    "result = {label: f\"{value:.4f} ± {std:.4f}\" for label, value, std in zip(param_labels, posterior_samples[0].tolist(), posterior_std.tolist())}\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-round\n",
    "\n",
    "# 1. Setup embedding and inference\n",
    "sequence_net = LSTMEmbedding()\n",
    "embedding_net = HybridEmbedding(seq_embed_net=sequence_net)\n",
    "density_estimator = posterior_nn(model='maf', embedding_net=embedding_net)\n",
    "inference = NPE(prior=prior, density_estimator=density_estimator)\n",
    "\n",
    "# 2. Precompute normalization stats\n",
    "stats_list = []\n",
    "for _ in range(500):  # Smaller number is fine for estimating stats\n",
    "    theta_i = prior.sample()\n",
    "    stats_i = multiphase_tank_summary(theta_i.numpy())\n",
    "    stats_list.append(stats_i)\n",
    "stats_array = np.stack(stats_list)\n",
    "stats_mean = stats_array.mean(axis=0)\n",
    "stats_std = stats_array.std(axis=0) + 1e-8\n",
    "\n",
    "# 3. Wrap simulator\n",
    "simulator_wrapper = process_simulator(\n",
    "    multiphase_tank_hybrid,  # this should return {\"sequence\": ..., \"stats\": ...}\n",
    "    prior,\n",
    "    prior_returns_numpy\n",
    ")\n",
    "\n",
    "# 4. Define true observation\n",
    "true_theta = torch.tensor([[5e4, 0.1, 0.005, 0.03]])\n",
    "x_o = simulator_wrapper(true_theta)  # dict → properly handled by sbi if using custom embedder\n",
    "\n",
    "\n",
    "# 5. Multi-round inference loop\n",
    "posterior = None\n",
    "\n",
    "for r in range(num_rounds):\n",
    "    print(f\"\\nRound {r+1}...\")\n",
    "\n",
    "    # Use posterior from previous round as proposal\n",
    "    proposal = posterior.set_default_x(x_o) if posterior is not None else prior\n",
    "\n",
    "    # Simulate using current proposal\n",
    "    theta, x = simulate_for_sbi(simulator_wrapper, proposal, num_simulations=5000)\n",
    "\n",
    "    # Train and build posterior\n",
    "    density_estimator = inference.append_simulations(theta, x, proposal=proposal).train()\n",
    "    posterior = inference.build_posterior(density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fc33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample posterior\n",
    "posterior_samples = posterior.sample((1000000,), x=x_o)\n",
    "\n",
    "param_labels = ['k_P', 'k_alpha', 'Q_g', 'Q_l']\n",
    "low = prior.base_dist.low.numpy()\n",
    "high = prior.base_dist.high.numpy()\n",
    "limits = [[l, h] for l, h in zip(low, high)]\n",
    "\n",
    "fig, ax = pairplot(\n",
    "    posterior_samples,\n",
    "    points=true_theta,\n",
    "    labels=param_labels,\n",
    "    limits=limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6f654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
